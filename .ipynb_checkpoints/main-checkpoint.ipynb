{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pre-work</h1>\n",
    "\n",
    "On my laptop I can't run the code for the entire dataframe, so we take a small part of the dataset to run on local (300 observations for Kaggle's training set, 30 for Kaggle's testing set).<br>\n",
    "<br>\n",
    "\n",
    "To run the entire dataset, we can use:\n",
    "- run on Google Cloud\n",
    "- use Columbia's Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Sets</h3>\n",
    "<h4>Kaggle Data:</h4>\n",
    "- Train : 19579 (text + label) (short texts)<br>\n",
    "- Test : 8392 (text only)\n",
    "<h4>Data Split:</h4>\n",
    "- TR1 (80%)<br>\n",
    "- TS1 (20%)\n",
    "<br>\n",
    "<h3>Features</h3>\n",
    "<h4>Building of features:</h4>\n",
    "- Bunch of meta-features<br>\n",
    "- TF_IDF: 1, 2, 3 grams<br>\n",
    "- Tokens : Words, Charaters, Pos_Tagging<br>\n",
    "- Vader : Positivness/Negativness<br>\n",
    "- Some others (Choice of character names, topic, Emotion percentageâ€¦)<br>\n",
    "- Normalization of the features\n",
    "\n",
    "<h3>Pipelines</h3>\n",
    "\n",
    "<h4>Dimensionality Reduction:</h4>\n",
    "- Size: 10, 20, quarter, half\n",
    "<h4>Methodology:</h4>\n",
    "- Univariate feature selection<br>\n",
    "- Principal Component Analysis\n",
    "<h4>Models:</h4>\n",
    "- Linear Regression<br>\n",
    "- K Neighbors Classifier<br>\n",
    "- Decision Tree Classifier<br>\n",
    "- GaussianNB<br>\n",
    "- Gradient Boosting Classifier<br>\n",
    "- AdaBoostClassifier<br>\n",
    "- Extra Trees Classifier<br>\n",
    "- Random Forest Classifier<br>\n",
    "- Calibrated BernoulliNB<br>\n",
    "- Calibrated Huber<br>\n",
    "\n",
    "<h4>Selection:</h4>\n",
    "- Cross validation over the TR1 dataframe(k=10)<br>\n",
    "- Selecting the 10 best pipelines in average<br>\n",
    "- For those 10, training over the whole TR1 and then test over the whole TS1<br>\n",
    "- Selecting the best in log_los\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install nltk\n",
    "#!pip3 install seaborn\n",
    "#!pip3 install langdetect\n",
    "#!pip3 install wordcloud\n",
    "#!pip3 install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import os\n",
    "import itertools\n",
    "from nltk import sent_tokenize, word_tokenize, FreqDist\n",
    "from nltk.data import load\n",
    "from langdetect import detect\n",
    "from math import ceil\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn import metrics, model_selection, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier)\n",
    "from sklearn.metrics import log_loss, fbeta_score, make_scorer, confusion_matrix, roc_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from multiprocessing.dummy import Pool  \n",
    "from sklearn import datasets # toy datasets\n",
    "start_time = datetime.now()\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Processing functions</h2>\n",
    "<h3>Normalization Function</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Normalization over number of words</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization_word(var):\n",
    "    \"\"\"Returns number of words.\"\"\"\n",
    "    words = nltk.Text(word_tokenize(((var))))\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Normalization by number of Sentences</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization_sentence(var):\n",
    "    \"\"\"Returns number of sentences.\"\"\"\n",
    "    sentences = nltk.Text(sent_tokenize(var))\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Normalization by number of Characters</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization_character(var):\n",
    "    \"\"\"Returns number of characters.\"\"\"\n",
    "    return len(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cleaning Function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning(var):\n",
    "    \"\"\"Take a string. Returns a string with only lowercase letters and the space between words.\"\"\"\n",
    "    plain_string = \"\"\n",
    "    for x in var:\n",
    "        x = x.lower()\n",
    "        if (('a' <= x and x <= 'z') or x == ' '):\n",
    "            plain_string += x\n",
    "        elif x == '\\'': # any apostrophes(') are replaced by a space \n",
    "            plain_string += ' '\n",
    "    while '  ' in plain_string: # any multiple spaces are replaced by a single space\n",
    "        plain_string = plain_string.replace('  ', ' ')\n",
    "    return plain_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creation of the dataframes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "#train = train[train.author.notnull()]\n",
    "#train.to_csv(\"train_check.csv\")\n",
    "#data must be in same folder\n",
    "\n",
    "author_list = ['EAP', 'HPL', 'MWS']\n",
    "train.text= train.text.astype(str) # casts the type of the text column to str\n",
    "train.author = pd.Categorical(train.author)\n",
    "\n",
    "#train = train[0:300]  # size reduction for coding\n",
    "\n",
    "train_back_up = train.copy() # back-up used to define future dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just a test cell\n",
    "test = train['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word Clouds</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full text from the authors as an array\n",
    "eap = train[train.author==\"EAP\"][\"text\"].values\n",
    "hpl = train[train.author==\"HPL\"][\"text\"].values\n",
    "mws = train[train.author==\"MWS\"][\"text\"].values\n",
    "\n",
    "# full text from the authors as a string\n",
    "eap_s = \" \".join(eap)\n",
    "hpl_s = \" \".join(hpl)\n",
    "mws_s = \" \".join(mws)\n",
    "\n",
    "# full PLAIN text (i.e. no capital and punctuation) from the authors as a string\n",
    "eap_s_c = cleaning(eap_s)\n",
    "hpl_s_c = cleaning(hpl_s)\n",
    "mws_s_c = cleaning(mws_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the whole text\n",
    "list_text = [eap_s_c, hpl_s_c, mws_s_c]\n",
    "\n",
    "# read the mask image\n",
    "# taken from http://www.stencilry.org/\n",
    "eap_mask = np.array(Image.open(\"eap_mask.jpg\"))\n",
    "hpl_mask = np.array(Image.open(\"hpl_mask.jpg\"))\n",
    "mws_mask = np.array(Image.open(\"mws_mask.jpg\"))\n",
    "list_mask = [eap_mask, hpl_mask, mws_mask]\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "for i in range(3):\n",
    "    wc = WordCloud(background_color=\"white\", max_words=2000, mask=list_mask[i],\n",
    "                   stopwords=stopwords)\n",
    "    # generate word cloud\n",
    "    wc.generate(list_text[i])\n",
    "    # show\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title('Wordcloud of author ' + author_list[i])\n",
    "    plt.axis(\"off\")\n",
    "    plt.figure()\n",
    "    # store to file\n",
    "    wc.to_file(\"{}_wordcloud.png\".format(author_list[i]))\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Splitting the training set.</h1>\n",
    "<br>\n",
    "Because we have only two dataset, one for training and the other for the Kaggle test. We need to split our 'Kaggle training' set (called t0) into training (called tr1) (in the sense of the training of our predictive model) and testing set (called ts1) (in the sense of testing our models, and not be tested by Kaggle !).<br>\n",
    "<br>\n",
    "We shall notice we will choose the best classifier only with the tr1 DataSet. Then, we will test this classifier with ts1 to see if we over-fitted over tr1.<br>\n",
    "<br>\n",
    "\n",
    "Once we have chosen the classifier and checked for over-fitting, we will train the chosen classifier over tr1 and ts1. We will use this to predict over the \"Kaggle Test Dataset\" and create our submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr1, ts1 = train_test_split(t0, test_size=20/80)\n",
    "\n",
    "tr1.to_csv(\"tr1.csv\", index = False)\n",
    "ts1.to_csv(\"ts1.csv\", index = False)\n",
    "\n",
    "# 20% to tr1 and 80% to ts1\n",
    "\n",
    "print(\"Number of sentences in Kaggle train set : \"+str(len(t0)))\n",
    "print(\"Number of sentences in train set tr1: \"+str(len(tr1)))\n",
    "print(\"Number of sentences in test set ts1: \"+str(len(ts1)))\n",
    "print(len(tr1) / len(t0))\n",
    "print(len(ts1) / len(t0))\n",
    "print((len(tr1) / len(t0)) + (len(ts1) / len(t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Meta Features</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Average sentence length (in characters)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_character(var):\n",
    "    \"\"\"Takes a string returns an int (average sentence length in characters).\"\"\"\n",
    "    return len(var) / normalization_sentence(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Average sentence length (in words)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_sentence(var):\n",
    "    \"\"\"Takes a string and returns an int (average sentence length in words).\"\"\"\n",
    "    return len(var.split()) / normalization_sentence(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Average characters per word</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_word(var):\n",
    "    \"\"\"Takes a string and returns an int (average characters per word). Excludes punctuations.\"\"\"\n",
    "    return len(var.split()) / normalization_word(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Punctuation density</h3>\n",
    "Now we take into consideration differences in punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_coma(var):\n",
    "    \"\"\"Takes a string and returns the ratio of punctuations to characters.\"\"\"\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == ',':\n",
    "            cpunc += 1\n",
    "    return cpunc / normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_point(var):\n",
    "    \"\"\"Takes a string and returns the ratio of periods(.) to characters.\"\"\"\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == '.':\n",
    "            cpunc += 1\n",
    "    return cpunc / normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_colon(var):\n",
    "    \"\"\"Takes a string and returns the ratio of colons(:) to characters.\"\"\"\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == ':':\n",
    "            cpunc += 1\n",
    "    return cpunc / normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_semicolon(var):\n",
    "    \"\"\"Takes a string and returns the ratio of semicolons(;) to characters.\"\"\"\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == ';':\n",
    "            cpunc += 1\n",
    "    return cpunc / normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_interro(var):\n",
    "    \"\"\"Takes a string and returns the ratio of question marks(?) to characters.\"\"\"\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == '?':\n",
    "            cpunc += 1\n",
    "    return cpunc / normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_expl(var):\n",
    "    \"\"\"Takes a string and returns the ratio of exclamation points(!) to characters.\"\"\"\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == '!':\n",
    "            cpunc += 1\n",
    "    return cpunc / normalization_character(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Percentage of unique words per sentence</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocabulary_sentence(var):\n",
    "    \"\"\"Takes a string and returns the ratio of different words to all words.\"\"\"\n",
    "    var = nltk.Text(sent_tokenize(var))\n",
    "    vocabulary_list = []\n",
    "    for c in var:\n",
    "        if normalization_word(c) != 0:\n",
    "            vacabulary_count_sentence = len({x.lower() for x in word_tokenize(cleaning(c))})\n",
    "            vocabulary_list.append(vacabulary_count_sentence / normalization_word(c))\n",
    "    return np.mean(vocabulary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Stopword percentage</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_stopword(var):\n",
    "    \"\"\"Takes a string and returns the ratio of stopwords to all words.\"\"\"\n",
    "    cs = 0\n",
    "    for x in nltk.Text(word_tokenize(var)):\n",
    "        if x in STOPWORDS:\n",
    "            cs += 1\n",
    "    return cs/normalization_word(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Noun Density</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_noun(var):\n",
    "    \"\"\"Takes a string and returns the ratio of nouns to all words.\"\"\"\n",
    "    l = []\n",
    "    for x in nltk.pos_tag(word_tokenize(var)):\n",
    "        if x[1][0:2] == 'NN': # all noun tags start with NN\n",
    "            l.append(x)\n",
    "    return len(l)/normalization_word(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Verb Density</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_verb(var):\n",
    "    \"\"\"Takes a string and returns the ratio of verbs to all words.\"\"\"\n",
    "    l = []\n",
    "    for x in nltk.pos_tag(word_tokenize(var)):\n",
    "        if x[1][0:2] == 'VB': # all verb tags start with VB\n",
    "            l.append(x)\n",
    "    return len(l)/normalization_word(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Adjective Density</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_adjective(var):\n",
    "    \"\"\"Takes a string and returns the ratio of adjectives to all words.\"\"\"\n",
    "    l = []\n",
    "    for x in nltk.pos_tag(word_tokenize(var)):\n",
    "        if x[1][0:2] == 'JJ': # all adjective tags start with JJ\n",
    "            l.append(x)\n",
    "    return len(l) / normalization_word(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Adjective to noun ratio</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjective_to_noun(var):\n",
    "    \"\"\"Takes a string and returns the ratio of adjectives to nouns.\"\"\"\n",
    "    return density_adjective(var) / (density_noun(var) + 0.5) # add 0.5 to avoid division by 0 error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Emphases on Words or Phrases</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_emph(var):\n",
    "    \"\"\"Takes a string and returns the usage count of emphases using double quotes.\"\"\"\n",
    "    emph_trig_words = 'word called the a their my his her for that those like of words'.split() \n",
    "    emph_count = 0\n",
    "    var = var.lower()\n",
    "    for word in emph_trig_words:\n",
    "        emph_count += var.count('{} \"'.format(word))\n",
    "    return emph_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dialogues Breaks</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_dial_break(var):\n",
    "    \"\"\"\n",
    "    Takes a string and returns the count of dialogue breaks used.\n",
    "    Example of a dialogue break: (\"D ,\" replied Dupin, \"is a desperate man, and a man of nerve.)\n",
    "    \"\"\"\n",
    "    return var.count(\", \\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dialogues</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_dblqt(var):\n",
    "    \"\"\"Takes a string and returns the count of sets of double quotes\"\"\"\n",
    "    return ceil(var.count('\"')/2) # ceil function rounds up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_dial(var):\n",
    "    \"\"\"\n",
    "    Takes a string and returns the count of dialogues used. Assumption is that if in double quotes but not an\n",
    "    emphasis, then it is a dialogue.\n",
    "    \"\"\"\n",
    "    return count_dblqt(var) - count_emph(var) - count_dial_break(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def break_to_dial_ratio(var):\n",
    "    \"\"\"Take sa string and returns the ratio of dialogue breaks to dialogues.\"\"\"\n",
    "    if not count_dial(var):\n",
    "        return 0\n",
    "    return count_dial_break(var) / count_dial(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feminine and Masculine Words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_fem(var):\n",
    "    \"\"\"Takes a string and returns the count of feminine words.\"\"\"\n",
    "    fem_words = 'she her woman herself girl women lady queen princess daughter madam madame wife'.split()\n",
    "    fem_count = 0\n",
    "    var = cleaning(var)\n",
    "    for word in var.split():\n",
    "        if word in fem_words:\n",
    "            fem_count += 1\n",
    "    return fem_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_mas(var):\n",
    "    \"\"\"Takes a string and returns the count of masculine words.\"\"\"\n",
    "    mas_words = 'he his man mr himself boy men gentleman gentlemen king prince son sir husband'.split()\n",
    "    mas_count = 0\n",
    "    var = cleaning(var)\n",
    "    for word in var.split():\n",
    "        if word in mas_words:\n",
    "            mas_count += 1\n",
    "    return mas_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fem_to_mas_ratio(var):\n",
    "    \"\"\"Takes a string and returns the ratio of feminine words to masculine words.\"\"\"\n",
    "    fem_count = count_fem(var)\n",
    "    mas_count = count_mas(var)\n",
    "    if fem_count and not mas_count:\n",
    "        fem_mas_ratio = 1\n",
    "    elif not fem_count and not mas_count:\n",
    "        fem_mas_ratio = 0\n",
    "    else:\n",
    "        fem_mas_ratio = fem_count / mas_count\n",
    "    return fem_mas_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([['EAP', eap_s], ['HPL', hpl_s], ['MWS', mws_s]])\n",
    "df.columns = ['author', 'text']\n",
    "\n",
    "for f in [count_emph, count_dial, count_dial_break, break_to_dial_ratio, count_fem, count_mas, fem_to_mas_ratio]:\n",
    "    df['norm_' + f.__name__] = df.text.apply(f)\n",
    "    \n",
    "for f in [count_emph, count_dial, count_dial_break, count_fem, count_mas]: # normalization of count_emph, count_dial, and count_dial_break\n",
    "    df['norm_' + f.__name__] = df['norm_' + f.__name__] / df.author.apply(lambda x: len(train[train.author == x]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in df.columns[2:]:\n",
    "    plt.style.use('seaborn')\n",
    "    sns.barplot(x='author', y=c, data=df)\n",
    "    plt.title('Meta {}'.format(c))\n",
    "    plt.savefig('Meta {}'.format(c))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True)\n",
    "fem_words = 'her she herself girl woman lady queen daughter madam madame wife women princess'.split()\n",
    "\n",
    "plt.suptitle('Normalized Feminine Word Count', y=1.02)\n",
    "\n",
    "xlmt = 0\n",
    "for auth in [(eap_s_c, 'EAP'), (hpl_s_c, 'HPL'), (mws_s_c, 'MWS')]:\n",
    "    for word in fem_words:\n",
    "        norm_count = auth[0].count(word) / len(train[train.author == auth[1]])\n",
    "        if norm_count > xlmt:\n",
    "            xlmt = norm_count\n",
    "\n",
    "for ax in ((ax1, eap_s_c, 'darkblue', 'EAP'), (ax2, hpl_s_c, 'darkgreen', 'HPL'), (ax3, mws_s_c, 'darkred', 'MWS')):\n",
    "    y = np.arange(len(fem_words))\n",
    "    x = [ax[1].count(word) / len(train[train.author == ax[3]]) for word in fem_words]\n",
    "    ax[0].barh(y, x, align='center',\n",
    "            color=ax[2], ecolor='black')\n",
    "    ax[0].set_yticks(y)\n",
    "    ax[0].set_yticklabels(fem_words)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].set_title(ax[3])\n",
    "    ax[0].set_xlim([0, xlmt])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('Meta feminine word count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True)\n",
    "mas_words = 'he his men man king son himself mr boy gentleman prince sir gentlemen husband mister'.split()\n",
    "\n",
    "plt.suptitle('Normalized Masculine Word Count', y=1.02)\n",
    "\n",
    "xlmt = 0\n",
    "for auth in [(eap_s_c, 'EAP'), (hpl_s_c, 'HPL'), (mws_s_c, 'MWS')]:\n",
    "    for word in mas_words:\n",
    "        norm_count = auth[0].count(word) / len(train[train.author == auth[1]])\n",
    "        if norm_count > xlmt:\n",
    "            xlmt = norm_count\n",
    "            \n",
    "for ax in ((ax1, eap_s_c, 'darkblue', 'EAP'), (ax2, hpl_s_c, 'darkgreen', 'HPL'), (ax3, mws_s_c, 'darkred', 'MWS')):\n",
    "    y = np.arange(len(mas_words))\n",
    "    x = [ax[1].count(word) / len(train[train.author == ax[3]]) for word in mas_words]\n",
    "    ax[0].barh(y, x, align='center',\n",
    "            color=ax[2], ecolor='black')\n",
    "    ax[0].set_yticks(y)\n",
    "    ax[0].set_yticklabels(mas_words)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].set_title(ax[3])\n",
    "    ax[0].set_xlim([0, xlmt])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('Meta masculine word count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Building of the Dataframe of Metadata</h3>\n",
    "\n",
    "This is the list of all the meta data we are going to use to train our classifiers. We are also going to use bag of words. This could be modified if we add meta data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_meta1(dataframe):\n",
    "    \"\"\"Builds a dataframe that shows the author and the first set of meta-data related to the sentence.\"\"\"\n",
    "    df_meta = dataframe.copy()\n",
    "    for f in [count_emph, count_dial, count_dial_break, break_to_dial_ratio, count_fem, count_mas, fem_to_mas_ratio]:\n",
    "        df_meta[f.__name__] = df_meta.text.apply(f)\n",
    "    \n",
    "    del df_meta['text']\n",
    "    if 'author' in df_meta.columns:\n",
    "        del df_meta['author']\n",
    "    \n",
    "    return df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_meta2(dataframe):\n",
    "    \"\"\"Builds a dataframe that shows the author and the other meta-data related to the sentence.\"\"\"\n",
    "    df_meta = dataframe.copy()\n",
    "    list_meta = [length_character,\n",
    "             length_sentence,\n",
    "             length_word,  \n",
    "             vocabulary_sentence,\n",
    "             density_stopword,\n",
    "             density_noun,\n",
    "             density_verb,\n",
    "             density_adjective,\n",
    "             adjective_to_noun,\n",
    "             density_coma,\n",
    "             density_point,\n",
    "             density_colon,\n",
    "             density_semicolon,\n",
    "             density_interro,\n",
    "             density_expl\n",
    "            ]\n",
    "    for f in list_meta:\n",
    "        df_meta[f.__name__] = df_meta.text.apply(f)\n",
    "    return df_meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for c in datameta2.columns[3:]:\n",
    "    plt.style.use('seaborn')\n",
    "    sns.boxplot(x='author', y=c, data=datameta2)\n",
    "    plt.title('Meta {}'.format(c))\n",
    "    plt.savefig('Meta {}'.format(c))\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Features</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>First Word</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_word_type(sents):\n",
    "    \"\"\"Takes a list of sentences and returns a list of the word type used to start each element.\"\"\"\n",
    "    res = list()\n",
    "    for sent in sents:\n",
    "        typ = nltk.pos_tag(sent.split())\n",
    "        i = 0\n",
    "        j = 0\n",
    "        word = typ[i][0].lower()\n",
    "        while i < len(sent.split()):\n",
    "            if j < len(word) and word[j] >= 'a' and word[j] <= 'z':\n",
    "                res.append(typ[i][1])\n",
    "                break\n",
    "            i += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_word(dataframe):\n",
    "    \"\"\"Takes a dataframe and creates a column per word type with a value of 1 on the word type used.\"\"\"\n",
    "    df = dataframe.copy()\n",
    "    elements = ['CC','CD','DT','EX','IN','JJ','JJR','JJS','LS','MD','NN','NNP','NNS','PDT','PRP','PRP$','RB','RBR','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WRB']\n",
    "    for element in elements:\n",
    "        column='first_word_'+element\n",
    "        df[column]=0\n",
    "        \n",
    "    for i in range(len(df)):\n",
    "        sents = nltk.Text(sent_tokenize(df['text'].iloc[i]))\n",
    "        first_words = first_word_type(sents)\n",
    "        for word in first_words:\n",
    "            column = 'first_word_' + str(word)\n",
    "            try:\n",
    "                df[column].iloc[i] += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    del df['text']\n",
    "    if 'author' in df.columns:\n",
    "        del df['author']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatext1 = first_word(tr1)\n",
    "datatext1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>First Two Words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twofirst_words_type(sents):\n",
    "    \"\"\"Takes a list of sentences and returns a list of the 2 word types used to start each element.\"\"\"\n",
    "    res = list()\n",
    "    for sent in sents:\n",
    "        if len(sent.split()) > 1:\n",
    "            typ = nltk.pos_tag(sent.split())\n",
    "            res.append([typ[0][1],typ[1][1]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twofirst_word(dataframe):\n",
    "    \"\"\"Takes a dataframe and creates a column per word type pair with a value of 1 on the word type pair used.\"\"\"\n",
    "    df = dataframe.copy()\n",
    "    for i in range(len(df)):\n",
    "        sents = nltk.Text(sent_tokenize(df['text'].iloc[i]))\n",
    "        twofirst_words = twofirst_words_type(sents)\n",
    "        for twoword in twofirst_words:\n",
    "            column = 'first_two_' + str(twoword)\n",
    "            try:\n",
    "                df[column].iloc[i] += 1\n",
    "            except:\n",
    "                df[column] = 0\n",
    "                df[column].iloc[i] = 1\n",
    "                \n",
    "    del df['text']\n",
    "    if 'author' in df.columns:\n",
    "        del df['author']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatext2 = twofirst_word(tr1)\n",
    "datatext2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Last Word</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_word_type(sents):\n",
    "    \"\"\"Takes a list of sentences and returns a list of the word type used to end each element.\"\"\"\n",
    "    res= list()\n",
    "    for sent in sents:\n",
    "        typ = nltk.pos_tag(sent.split())\n",
    "        i = 0\n",
    "        j = 0\n",
    "        word = typ[-i][0].lower()\n",
    "        while i < len(sent.split()):\n",
    "            if j < len(word) and word[j] >= 'a' and word[j] <= 'z':\n",
    "                res.append(typ[-i][1])\n",
    "                break\n",
    "            i += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_word(dataframe):\n",
    "    \"\"\"Takes a dataframe and creates a column per word type with a value of 1 on the word type used.\"\"\"\n",
    "    df = dataframe.copy()\n",
    "    elements = ['FW','IN','JJ','NN','NNP','NNS','RB','RBS','RP','VB','VBD','VBN','VBP','VBZ']\n",
    "    for element in elements:\n",
    "        column='last_word_'+element\n",
    "        df[column]=0\n",
    "        \n",
    "    for i in range(len(df)):\n",
    "        sents = nltk.Text(sent_tokenize(df['text'].iloc[i]))\n",
    "        last_words = last_word_type(sents)\n",
    "        for word in last_words:\n",
    "            column = 'last_word_' + str(word)\n",
    "            try:\n",
    "                df[column].iloc[i] += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    del df['text']\n",
    "    if 'author' in df.columns:\n",
    "        del df['author']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatext3 = last_word(tr1)\n",
    "datatext3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Last Two Words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twolast_words_type(sents):\n",
    "    \"\"\"Takes a list of sentences and returns a list of the word types used to end each element.\"\"\"\n",
    "    res = list()\n",
    "    for sent in sents:\n",
    "        if len(sent.split()) > 1:\n",
    "            typ = nltk.pos_tag(sent.split())\n",
    "            res.append([typ[-2][1], typ[-1][1]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twolast_word(dataframe):\n",
    "    \"\"\"Takes a dataframe and creates a column per word type pair with a value of 1 on the word type pairs used.\"\"\"\n",
    "    df=dataframe.copy()\n",
    "    for i in range(len(df)):\n",
    "        sents=nltk.Text(sent_tokenize(df['text'].iloc[i]))\n",
    "        twolast_words = twolast_words_type(sents)\n",
    "        for twoword in twolast_words:\n",
    "            column='last_two_' + str(twoword)\n",
    "            try:\n",
    "                df[column].iloc[i]=df[column].iloc[i]+1\n",
    "            except:\n",
    "                df[column]=0\n",
    "                df[column].iloc[i]=1\n",
    "                \n",
    "    del df['text']\n",
    "    if 'author' in df.columns:\n",
    "        del df['author']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatext4 = twolast_word(tr1)\n",
    "datatext4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Foreign Languages </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_sent(sents):\n",
    "    \"\"\"Takes a list of sentences and returns a list of the languages detected for each sentence.\"\"\"\n",
    "    res = list()\n",
    "    for sent in sents:\n",
    "        try:\n",
    "            res.append(detect(sent))\n",
    "        except:\n",
    "            pass        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language(dataframe):\n",
    "    \"\"\"Takes a dataframe and creates a column per language with a value of 1 on the languages used.\"\"\"\n",
    "    df = dataframe.copy()\n",
    "    languages=['af','ca','cy','da','de','es','et','fi','fr','hu','ind','it','nl','no','pl','pt','ro','sk','sl','so','sv','tl','tr','vi']\n",
    "    for language in languages:\n",
    "        df[language]=0\n",
    "        \n",
    "    for i in range(len(df)):\n",
    "        sents = nltk.Text(sent_tokenize(df['text'].iloc[i]))\n",
    "        n = len(sents)\n",
    "        languages = language_sent(sents)\n",
    "        for language in languages:\n",
    "            if language != 'id':\n",
    "                column = language\n",
    "                try:\n",
    "                    df[column].iloc[i] = df[column].iloc[i] + 1 / n\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                column='ind'\n",
    "                try:\n",
    "                    df[column].iloc[i] = df[column].iloc[i] + 1 / n\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    del df['text']\n",
    "    if 'author' in df.columns:\n",
    "        del df['author']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatext5 = language(tr1)\n",
    "datatext5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Emotions </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function needs to have the file \"NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\" in the same folder.\n",
    "Based on NRC data: 8 emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nrc_data():\n",
    "    \"\"\"Builds an emotion dictionary from the NRC emotion lexicon.\"\"\"\n",
    "    nrc = \"NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\"\n",
    "    count = 0\n",
    "    emotion_dict = dict()\n",
    "    with open(nrc,'r') as f:\n",
    "        all_lines = list()\n",
    "        for line in f:\n",
    "            if count < 46:\n",
    "                count += 1\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if int(line[2]) == 1:\n",
    "                if emotion_dict.get(line[0]):\n",
    "                    emotion_dict[line[0]].append(line[1])\n",
    "                else:\n",
    "                    emotion_dict[line[0]] = [line[1]]\n",
    "    return emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emotions(dataframe):\n",
    "    \"\"\"Takes a dataframe and creates a column per emotion with a count on the emotions used.\"\"\"\n",
    "    df = dataframe.copy()\n",
    "    emotions=['positive','anger','disgust','fear','negative','sadness','anticipation','joy','surprise','trust']\n",
    "    for emotion in emotions:\n",
    "        df[emotion]=0\n",
    "    \n",
    "    \n",
    "    emotion_dic = get_nrc_data()\n",
    "    for i in range(len(df)):\n",
    "        words = df['text'].iloc[i].split()\n",
    "        n = len(words)\n",
    "        for word in words:\n",
    "            if word in emotion_dic:\n",
    "                for emotion in emotion_dic[word]:\n",
    "                    column = emotion\n",
    "                    try:\n",
    "                        df[column].iloc[i] = df[column].iloc[i] + 1 / n\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "    del df['text']\n",
    "    if 'author' in df.columns:\n",
    "        del df['author']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatext6 = emotions(tr1)\n",
    "datatext6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Counting of words (a.k.a bag of words)</h3>\n",
    "\n",
    "Instead of word/ n-grams frequencies, we are going to use tf_idf.\n",
    "\n",
    "Some information about tf_idf: https://buhrmann.github.io/tfidf-analysis.html\n",
    "\n",
    "Here, we don't detail for each N. Instead, we'll run the tf_idf for all the n-grams possible and the we'll take the best features. Our aim is to reduce the size/dimensions of our dataset's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_tag(var):\n",
    "    \"\"\"Transforms a string to a string of pos_tag\"\"\"\n",
    "    inpt = nltk.pos_tag(word_tokenize(var))\n",
    "    unzipped = zip(*inpt )\n",
    "    return ' '.join([*list(unzipped)[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generation of the tf_idf counting dataFrame</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def counting_a(a, analysis):\n",
    "    \"\"\"\n",
    "    Generates the tf-idf counting dataframe. First argument is the n of n-gram. Analysis type is 'word', 'char',\n",
    "    token_pos', etc.\n",
    "    \"\"\"\n",
    "    df_train = tr1.copy()\n",
    "    df_test  = ts1.copy()\n",
    "    \n",
    "    #if we are counting words:\n",
    "    if analysis == \"word\" or analysis == \"char\": \n",
    "        \n",
    "        #check the CountVectorizer doc\n",
    "        #we create a Countvectorizer, called bow_transformer\n",
    "        bow_transformer = CountVectorizer(analyzer = analysis,\n",
    "                                      lowercase = True, #we don't care about place in sentence\n",
    "                                      ngram_range = (a, a),\n",
    "                                      stop_words='english')\n",
    "\n",
    "        #we use bow_transformer to fit and transform our training set\n",
    "        messages_bow = bow_transformer.fit_transform(df_train['text'])\n",
    "        \n",
    "        #we use bow_transformer to transform our test set. \n",
    "        #We do not need to train if first because the fitting would recompute the idf, we don't want that\n",
    "        messages_bow_test = bow_transformer.transform(df_test['text'])\n",
    "    \n",
    "    #if we are counting POS:    \n",
    "    elif analysis == \"token_pos\":\n",
    "        \n",
    "        #this is the punctuation we want to keep\n",
    "        punctuation = r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'|\\.|\\,|\\;|\\:|\\$|\\(|\\)|\\--|\\&|\\``|\\'' + PRP$ + WP$\"\n",
    "        #we create a Countvectorizer, called bow_transformer\n",
    "        bow_transformer = CountVectorizer(analyzer = 'word',\n",
    "                                          lowercase = False, #we DO care about place in sentence\n",
    "                                          ngram_range = (a, a),\n",
    "                                          token_pattern =  punctuation, #we DO care about punctuation\n",
    "                                          stop_words='english')\n",
    "        \n",
    "        #we use the transform_tag function to transform the sentence in a sentence of pos tag        \n",
    "        #we use bow_transformer to fit and transform our training set\n",
    "        messages_bow = bow_transformer.fit_transform(df_train['text'].apply(transform_tag))\n",
    "        #we use bow_transformer to transform our test set\n",
    "        #We do not need to train if first because the fitting would recompute the idf, we don't want that\n",
    "        messages_bow_test = bow_transformer.transform(df_test['text'].apply(transform_tag))\n",
    "\n",
    "        \n",
    "    #this is the DataFrame Concerning the regular counting of words\n",
    "    \n",
    "    ##from regular counting to tf idf transformation coefficient\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "    messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "    messages_tfidf_test = tfidf_transformer.transform(messages_bow_test)\n",
    "    \n",
    "    names = bow_transformer.get_feature_names()\n",
    "    \n",
    "    return (messages_tfidf, names, messages_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_word, name_word, mat_word_test = counting_a(2,'word')\n",
    "\n",
    "print ('Shape of Sparse Matrix: ', mat_word.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_word.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_word.nnz /\n",
    "                             (mat_word.shape[0] * mat_word.shape[1])))\n",
    "       \n",
    "print(' ')\n",
    "print ('Shape of Sparse Matrix Test: ', mat_word_test.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_word_test.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_word_test.nnz /\n",
    "                             (mat_word_test.shape[0] * mat_word_test.shape[1])))\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get a matrix with <u>roughly 150,000</u>  features (for the whole dataset. It's really heavy and we get a sparse matrix. Our goal is now to reduce the size of this matrix by getting the TOP-N features issued from the tf_idf.<br>\n",
    "<br>\n",
    "And we can create so both the matrix of training set. On which the TF_IDF is trained. <br>\n",
    "And the matrix test, which is created with no-fit on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reduction of the number of features to N</h4>\n",
    "\n",
    "The next cell will be called in all the bag of words parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    \"\"\"Gets top n tf-idf values in row and return them with their corresponding feature names.\"\"\"\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    \"\"\"Returns the top n tf-df features in a specific document (i.e. matrix row)\"\"\"\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    \"\"\"\n",
    "    Returns the top n features that on average are most important amongst documents in rows.\n",
    "    Indentified by indices in grp_ids.\n",
    "    \"\"\"\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    \"\"\"\n",
    "    Return a list of dfs, where each df holds top_n features and their mean tfidf value.\n",
    "    Calculated across documents with the same class label.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tfidf_classfeats_h(dfs, name = 'default'):\n",
    "    \"\"\"Plot the data frames returned by the function plot_tfidf_classfeats().\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"label = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center', color='#3F5D7D')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    #this line is for saving as picture\n",
    "    plt.savefig(name)\n",
    "    #this line is for showing\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Dimensionality reduction for bag of words (example for 2-grams)</h4>\n",
    "We reduce by taking the TOP-N per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_word = top_feats_by_class(mat_word, tr1.author, name_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_tfidf_classfeats_h(alpha_word, 'bi_gram_word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building of bag of word and feature vectors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_bag_a(a, analysis, top_n = 50):\n",
    "    df_bag = tr1.copy()\n",
    "    df_bag_test = ts1.copy()\n",
    "    \n",
    "    build = counting_a(a, analysis)\n",
    "    \n",
    "    alpha = top_feats_by_class(build[0], df_bag.author, build[1], top_n = top_n)\n",
    "\n",
    "    a = list(alpha[0].feature.values)\n",
    "    b = list(alpha[1].feature.values)\n",
    "    c = list(alpha[2].feature.values)\n",
    "    bag = set(a + b + c)\n",
    "\n",
    "\n",
    "    for w in bag:\n",
    "        vec = build[0][:, build[1].index(w)].toarray()\n",
    "        df_bag[w] = vec\n",
    "\n",
    "        vec_test = build[2][:, build[1].index(w)].toarray()\n",
    "        df_bag_test[w] = vec_test\n",
    "        \n",
    "    df_bag = df_bag.drop(labels = ['text','author'], axis = 1)\n",
    "    df_bag_test = df_bag_test.drop(labels = ['text','author'], axis = 1)\n",
    "\n",
    "        \n",
    "    return df_bag, df_bag_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we get the top N-grams of words. Our problem is the depedency on the topic. We need to produce other features which are less dependant on the topic. There are sereveral possibility. Let's detail: <br>\n",
    "- Character Counting<br>\n",
    "- Pos_tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Counting of Character (a.k.a bag of character)</h2>\n",
    "Here, we count the use of some caracter, and n-grams of caracter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Generation of the tf-idf counting dataFrame</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_char, name_char, mat_char_test = counting_a(3, 'char')\n",
    "\n",
    "print ('Shape of Sparse Matrix: ', mat_char.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_char.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_char.nnz /\n",
    "                             (mat_char.shape[0] * mat_char.shape[1])))\n",
    "print(' ')\n",
    "print ('Shape of Sparse Matrix Test: ', mat_char_test.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_char_test.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_char_test.nnz /\n",
    "                             (mat_char_test.shape[0] * mat_char_test.shape[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Dimensionnality reduction for bag of characters example for 3 gram</h3>\n",
    "We reduce by taking the TOP-N per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_char = top_feats_by_class( mat_char, tr1.author, name_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_tfidf_classfeats_h(alpha_char, 'tr_gram_char')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Counting of POS Tag (a.k.a bag of Tag)</h2>\n",
    "\n",
    "Here, we count the use of some caracter, and n-grams of caracter.<br>\n",
    "So we have, some non-topic sensitive features.<br>\n",
    "But we can produce an other type of feature based on the POS_tagging.\n",
    "\n",
    "POS tag features. \n",
    "\n",
    "We will check the occurence of the elements from the Upenn Tagset. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('tagsets')\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "#list of all the possible tag names\n",
    "#print(\"|\\\\\".join(list(tagdict)))\n",
    "\n",
    "#this is the list of the different tokens we will use.\n",
    "#len(tagdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Generation of the tf-idf counting dataFrame</h3>\n",
    "We had to adapt the arguments passed in the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_pos, name_pos, mat_pos_test = counting_a(4, 'token_pos')\n",
    "\n",
    "print ('Shape of Sparse Matrix: ', mat_pos.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_pos.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_pos.nnz /\n",
    "                             (mat_pos.shape[0] * mat_pos.shape[1])))\n",
    "\n",
    "print ('Shape of Sparse Test Matrix: ', mat_pos_test.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_pos_test.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_pos_test.nnz /\n",
    "                             (mat_pos_test.shape[0] * mat_pos_test.shape[1])))\n",
    "                             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dimensionnality reduction for bag of tags Example for 2 grams</h3>\n",
    "We reduce by taking the TOP-N per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_pos = top_feats_by_class(mat_pos, tr1.author, name_pos, top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_tfidf_classfeats_h(alpha_pos, \"four_gram_tag_pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Positivity/Negativity</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Weighted sentiment analysis using Vader</h4>\n",
    "Vader contains a list of 7500 features weighted by how positive or negative they are</h4>\n",
    "<br>It uses these features to calculate stats on how positive, negative and neutral a passage is<br>\n",
    "<br>And combines these results to give a compound sentiment (higher = more positive) for the passage<br>\n",
    "<br>Human trained on twitter data and generally considered good for informal communication<br>\n",
    "<br>10 humans rated each feature in each tweet in context from -4 to +4</h4>\n",
    "<br>Calculates the sentiment in a sentence using word order analysis</h4>\n",
    "<br>\"marginally good\" will get a lower positive score than \"extremely good\"\n",
    "<br>Computes a \"compound\" score based on heuristics (between -1 and +1)</h4>\n",
    "<br>Includes sentiment of emoticons, punctuation, and other 'social media' lexicon elements<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vader_comparison(texts):\n",
    "    headers = ['pos','neg','neu','compound']\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentences = sent_tokenize(texts)\n",
    "    pos = compound = neu = neg = 0\n",
    "    num_sentences = len(sentences)\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        pos += vs['pos'] / num_sentences\n",
    "        compound += vs['compound'] / num_sentences\n",
    "        neu += vs['neu'] / num_sentences\n",
    "        neg += vs['neg'] / num_sentences\n",
    "    return pos, neg, neu, compound\n",
    "\n",
    "def density_positive(var):\n",
    "    return vader_comparison(var)[0]\n",
    "\n",
    "def density_negative(var):\n",
    "    return vader_comparison(var)[1]\n",
    "\n",
    "def density_neutral(var):\n",
    "    return vader_comparison(var)[2]\n",
    "\n",
    "def density_compound(var):\n",
    "    return vader_comparison(var)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_sensi(dataframe):\n",
    "    df_sen = dataframe.copy() #change here to make sense\n",
    "    \n",
    "    df_sen[density_positive.__name__] = df_sen.text.apply(density_positive)\n",
    "    df_sen[density_negative.__name__] = df_sen.text.apply(density_negative)\n",
    "    df_sen[density_neutral.__name__] = df_sen.text.apply(density_neutral)\n",
    "    df_sen[density_compound.__name__] = df_sen.text.apply(density_compound)\n",
    "    \n",
    "    del df_sen['text']\n",
    "    if 'author' in df_sen.columns:\n",
    "        del df_sen['author']\n",
    "    return df_sen\n",
    "\n",
    "#build_sensi(ts1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatext7 = build_sensi(tr1)\n",
    "datatext7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tab = pd.merge(train, datatext7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in new_tab.columns[3:]:\n",
    "    plt.style.use('seaborn')\n",
    "    sns.boxplot(x = 'author', y = c, data = new_tab)\n",
    "    plt.title('Sensi {}'.format(c))\n",
    "    plt.savefig('Sensi {}'.format(c))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Fusion of the bunch of features.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we build here the 2 feature datasets (one for TR1, one for TS1)\n",
    "#should be adapted when we'll add features\n",
    "\n",
    "def build_bunch_tr1(dataframe):\n",
    "    list_df_tr = [build_meta2(dataframe),\n",
    "                  build_meta1(dataframe),\n",
    "                  build_sensi(dataframe),\n",
    "                  build_bag_a(1, 'word')[0],\n",
    "                  build_bag_a(2, 'word')[0],\n",
    "                  build_bag_a(3, 'word')[0],\n",
    "                  build_bag_a(1, 'char')[0],\n",
    "                  build_bag_a(2, 'char')[0],\n",
    "                  build_bag_a(3, 'char')[0],\n",
    "                  build_bag_a(4, 'char')[0],\n",
    "                  build_bag_a(5, 'char')[0],\n",
    "                  build_bag_a(6, 'char')[0],\n",
    "                  build_bag_a(7, 'char')[0],\n",
    "                  build_bag_a(1, 'token_pos')[0],\n",
    "                  build_bag_a(2, 'token_pos')[0],\n",
    "                  build_bag_a(3, 'token_pos')[0],\n",
    "                  build_bag_a(4, 'token_pos')[0],\n",
    "                  build_bag_a(5, 'token_pos')[0],\n",
    "                  build_bag_a(6, 'token_pos')[0],\n",
    "                  first_word(dataframe),\n",
    "                  last_word(dataframe),\n",
    "                  language(dataframe),\n",
    "                  emotions(dataframe)\n",
    "              ]\n",
    "    bunch = pd.merge(list_df_tr[0], list_df_tr[1])\n",
    "    \n",
    "    for i in range(2, len(list_df_tr)):\n",
    "        alpha = bunch\n",
    "        bunch = pd.merge(alpha, list_df_tr[i], on = 'id')\n",
    "    \n",
    "    return bunch\n",
    "\n",
    "#df_feat_tr1 = build_bunch_tr1(tr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_bunch_ts1(dataframe):\n",
    "    list_df_ts = [build_meta2(dataframe),\n",
    "                  build_meta1(dataframe),\n",
    "                  build_sensi(dataframe),\n",
    "                  build_bag_a(1,'word')[1],\n",
    "                  build_bag_a(2,'word')[1],\n",
    "                  build_bag_a(3,'word')[1],\n",
    "               \n",
    "                  build_bag_a(1,'char')[1],\n",
    "                  build_bag_a(2,'char')[1],\n",
    "                  build_bag_a(3,'char')[1],\n",
    "                  build_bag_a(4,'char')[1],\n",
    "                  build_bag_a(5,'char')[1],\n",
    "                  build_bag_a(6,'char')[1],\n",
    "                  build_bag_a(7,'char')[1],\n",
    "               \n",
    "                  build_bag_a(1, 'token_pos')[1],\n",
    "                  build_bag_a(2, 'token_pos')[1],\n",
    "                  build_bag_a(3, 'token_pos')[1],\n",
    "                  build_bag_a(4, 'token_pos')[1],\n",
    "                  build_bag_a(5, 'token_pos')[1],\n",
    "                  build_bag_a(6, 'token_pos')[1],\n",
    "                  first_word(dataframe),\n",
    "                  last_word(dataframe),\n",
    "                  language(dataframe),\n",
    "                  emotions(dataframe)\n",
    "                 ]\n",
    "\n",
    "    bunch = pd.merge(list_df_ts[0], list_df_ts[1])\n",
    "    \n",
    "    for i in range(2, len(list_df_ts)):\n",
    "        print(bunch.shape)\n",
    "        bunch = pd.merge(bunch, list_df_ts[i], on = 'id')\n",
    "    \n",
    "    return bunch\n",
    "#df_feat_ts1 = build_bunch_ts1(ts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Quantitative parameters transformation</h2>\n",
    "Quantitative parameters were transferred into a numerical vector with min-max normalization\n",
    "from 0 to 1. <br>\n",
    "<br>\n",
    "It is necessary to have normalized vectors before training our dataset to avoid a disequilibrium among the coefficients.\n",
    "\n",
    "http://blog.josephmisiti.com/help-commands-for-doing-machine-learning-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_normalization(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in list(df)[3:]:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        if max_value != min_value:\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "#build_normalization(df_feat_ts1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we have the normalized features matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prediction: Selection of best set of feature selection technique and prediction model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load and prepare data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "names = author_list\n",
    "dataframe_train = build_normalization(build_bunch_tr1(tr1))\n",
    "del dataframe_train[\"text\"]\n",
    "dataframe_train.to_csv('dataframe_train.csv', index=False)\n",
    "\n",
    "#dataframe_train = pd.read_csv(\"dataframe_train.csv\")\n",
    "array = dataframe_train.values\n",
    "X_train = array[:,2:]\n",
    "Y_train = array[:,1]\n",
    "\n",
    "names = author_list\n",
    "dataframe_test = build_normalization(build_bunch_ts1(ts1))\n",
    "del dataframe_test[\"text\"]\n",
    "dataframe_test.to_csv('dataframe_test.csv', index=False)\n",
    "\n",
    "#dataframe_test = pd.read_csv(\"dataframe_test.csv\")\n",
    "array = dataframe_test.values\n",
    "X_test = array[:,2:]\n",
    "Y_test = array[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Selection</h2><br>\n",
    "See:http://scikit-learn.org/stable/modules/feature_selection.html <br><br>\n",
    "Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.<br>\n",
    "<br>\n",
    "Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.<br>\n",
    "<br>\n",
    "Three benefits of performing feature selection before modeling your data are:<br>\n",
    "<br>\n",
    "- <b>Reduces Overfitting:</b> Less redundant data means less opportunity to make decisions based on noise.<br>\n",
    "    <br>\n",
    "- <b>Improves Accuracy:</b> Less misleading data means modeling accuracy improves.<br>\n",
    "    <br>\n",
    "- <b>Reduces Training Time:</b> Less data means that algorithms train faster.\n",
    "<br><br>I will not follow the paper here but rather I'll follow the following website: <br>\n",
    "\n",
    "https://machinelearningmastery.com/feature-selection-machine-learning-python/ <br>\n",
    "see also: https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here we will try a bunch of different methods:<br>\n",
    "- Univariate feature selection\n",
    "- Recursive feature elimination\n",
    "- L1-based feature selection\n",
    "- Tree-based feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of selected features</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Current number of features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('We have ' + str(np.shape(X_train)[1]) + ' features in total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reduction of  number of feaures</h4>\n",
    "The common possibilities are 10,20, quarter and half of total number of samples. <br>\n",
    "<br>\n",
    "For now, we only explore with quarter, tbd for the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_N = [10, 20, int(np.shape(X_train)[1]/4), int(np.shape(X_train)[1]/2)]\n",
    "\n",
    "print('We want to test our models with ' + str(list_N) + ' features extracted from the ' + str(np.shape(X_train)[1]) + ' previous features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Univariate feature selection</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sel_univ(N):\n",
    "    return SelectKBest(chi2, k=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Recursive feature elimination</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sel_rec(N):\n",
    "    model_rec = LogisticRegression()\n",
    "    return RFE(model_rec, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Principal Component Analysis</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sel_pca(N):\n",
    "    return PCA(n_components=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_selec(N):\n",
    "    selectioners = []  \n",
    "    selectioners.append(('S_UNIV_{}'.format(N), sel_univ(N)))\n",
    "    selectioners.append(('S_PCA_{}'.format(N), sel_pca(N)))\n",
    "    return selectioners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predictive Models</h2><br>\n",
    "So here we will try this bunch of feature models:\n",
    "\n",
    "- Linear Regression\n",
    "- K Neighbors Classifier\n",
    "- Decision Tree Classifier\n",
    "- GaussianNB\n",
    "- Gradient Boosting Classifier\n",
    "- AdaBoostClassifier\n",
    "- Extra Trees Classifier\n",
    "- Random Forest Classifier\n",
    "- Calibrated BernoulliNB\n",
    "- Calibrated Huber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_models(N):\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('GBC', GradientBoostingClassifier()))\n",
    "    models.append(('ABC', AdaBoostClassifier()))\n",
    "    models.append(('ETC', ExtraTreesClassifier()))\n",
    "    models.append(('RFC', RandomForestClassifier()))\n",
    "    models.append(('Calibrated BernoulliNB', CalibratedClassifierCV( BernoulliNB(alpha=0.03), method='isotonic')))\n",
    "    models.append(('Calibrated Huber', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=1e-4, max_iter=10000, tol=1e-4), method='sigmoid')))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pipeline</h2><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves two purposes here:<br>\n",
    "- Convenience and Encapsulation<br>\n",
    "        You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "- Joint parameter selection<br>\n",
    "        You can grid search over parameters of all estimators in the pipeline at once.\n",
    "- Safety<br>\n",
    "        Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pipes(N):\n",
    "    pipes = []\n",
    "    for sel in build_selec(N):\n",
    "        for mod in build_models(N):\n",
    "            pipes.append([sel, mod])\n",
    "    return pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Compare set of selection and predictive models</h2>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, we do ten times :<br>\n",
    "- First we train over 9/10 of the training set (TR1)\n",
    "- We test over the 1/10 remaining of the training set (TR1)<br>\n",
    "<br>\n",
    "But here we have a problem. Actually, the metric we need to mesure the models is 'neg_log_loss', because it is the one used by Kaggle. But it doesn't work and we don't know why. So, the metric used is accuracy. <br>\n",
    "<br>\n",
    "Then, we will take the 10 best models:<br>\n",
    "- We train over the whole train set(TR1)\n",
    "- We test over the whole test set (TS1)\n",
    "- we mesure the best with the neg_log_loss metric (here it's working)<br>\n",
    "Here the goal is to select the best with the Kaggle metric, but also we want to check if there is or not overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_list_models():\n",
    "    list_all = []\n",
    "    for n in list_N:\n",
    "        list_all.extend(build_pipes(n))\n",
    "    return list_all\n",
    "        \n",
    "all_pipes = build_list_models()\n",
    "\n",
    "all_pipes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_name_pipe = []\n",
    "\n",
    "for pipe in all_pipes:\n",
    "    feat = pipe[0]\n",
    "    model = pipe[1]    \n",
    "    list_name_pipe.append(feat[0] + ' ' + model[0])\n",
    "\n",
    "monitoring = pd.DataFrame( index = list_name_pipe, columns = ['started', 'start_time', 'ended', 'end_time', 'delta', 'mean', 'std', 'criterion'] )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_N(pipe):\n",
    "\n",
    "    intermediate_time = datetime.now()\n",
    "    seed = 7    \n",
    "    feat = pipe[0]\n",
    "    model = pipe[1]\n",
    "    \n",
    "    name = feat[0] + ' ' + model[0]\n",
    "    \n",
    "    monitoring['started'][name] = True\n",
    "    monitoring['start_time'][name] = intermediate_time\n",
    "\n",
    "    print('Go: ' + name + '\\n')\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    pipeline = Pipeline([feat, model])\n",
    "    scorer = 'neg_log_loss'\n",
    "    cv_results = model_selection.cross_val_score(pipeline,\n",
    "                                                 X_train,\n",
    "                                                 Y_train,\n",
    "                                                 cv=kfold,\n",
    "                                                 scoring = scorer)\n",
    "    results = cv_results\n",
    "    results_mean_var = (feat[0] + ' ' + model[0], cv_results.mean(), cv_results.var(), cv_results.mean()-0.5*cv_results.std())\n",
    "    #names_model.append(feat[0] + ' ' + model[0])\n",
    "    msg = \"%s: Mean=%f Stand Dev=(%f)\" % ('Selector: ' + feat[0] + ' Predictive model: ' + model[0], cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    end_time = datetime.now()\n",
    "    print('Running time: {}'.format(end_time - start_time))\n",
    "    print('Intermediate time: {}'.format(end_time - intermediate_time))\n",
    "    \n",
    "    monitoring['ended'][name] = True\n",
    "    monitoring['end_time'][name] = end_time\n",
    "    monitoring['delta'][name] = end_time-intermediate_time\n",
    "    monitoring['mean'][name] = cv_results.mean()\n",
    "    monitoring['std'][name] = cv_results.var()\n",
    "    monitoring['criterion'][name] = cv_results.mean()-0.5*cv_results.std()\n",
    "    monitoring.to_csv(\"monitoring.csv\")\n",
    "    return (results, results_mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "debut = datetime.now()\n",
    "\n",
    "pool = Pool() \n",
    "training_results = pool.map(eval_N, all_pipes)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "fin = datetime.now()\n",
    "print('Computation finished:' + str(fin - debut))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Selection of selection and predictive models</h2>\n",
    "<br> We take the three best estimators in mean. These will be tested other the 'TS1' test dataset. <br>\n",
    "Really not sure about me for the criterion. Because, the variance is important too. We should think about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_and_select_results_mean_var():\n",
    "    list_res = [] #total list of results\n",
    "    selection = [] #top ten in term of average \n",
    "                   #subject to be in the criterion in term of mean\n",
    "    \n",
    "    \n",
    "    for i in range(len(training_results)):\n",
    "        list_res.append((training_results[i][1]))\n",
    "        \n",
    "    list_res.sort(key=lambda tup: tup[3], reverse = True)\n",
    "    \n",
    "    selection = list_res[:10]\n",
    "\n",
    "    return selection\n",
    "\n",
    "#selected_feat_model = get_and_select_results_mean_var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_ten = pd.read_csv(\"monitoring.csv\", index_col=0)\n",
    "top_ten = top_ten.sort_values([\"criterion\"], ascending = False)\n",
    "top_ten = top_ten.index[:10]\n",
    "\n",
    "selected_feat_model = top_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing over the Testing dataset</h1> <br>\n",
    "Now, we have to test the top-10 models over the remaining training set. Our aim is to see how are they are behaviouring over an un-known dataset and select the best. We will also, and it's the most import ensure there is no overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>List of selected</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "best_pipes = []\n",
    "#print(build_pipes(10)[1][0][0])\n",
    "#print(selected_feat_model[2])\n",
    "for j in range(len(selected_feat_model)):\n",
    "    for n in list_N:    \n",
    "        for i in range(len(build_pipes(n))):\n",
    "            f = build_pipes(n)[i][0][0] + ' ' + build_pipes(n)[i][1][0]\n",
    "            if f == selected_feat_model[j][0]:\n",
    "                best_pipes.append((f, build_pipes(n)[i]))\n",
    "#print(len(best_pipes)) \n",
    "\n",
    "top_ten_pipes = []\n",
    "\n",
    "for p in all_pipes:\n",
    "    for s in top_ten:\n",
    "        if s == p[0][0] + ' ' + p[1][0]:\n",
    "            top_ten_pipes.append(p)\n",
    "len(top_ten_pipes)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion Matrix</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = author_list\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def final_plot_confusion(Y_test, Y_pred):\n",
    "\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    plt.savefig('conf_matrix_1')\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    plt.savefig('conf_matrix_2')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test over Top Pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss_list = []\n",
    "best_pipes = top_ten_pipes\n",
    "\n",
    "for i in range(len(best_pipes)):\n",
    "    name = best_pipes[i][0][0]+' '+best_pipes[i][1][0]\n",
    "    pipeline = Pipeline([best_pipes[i][0], best_pipes[i][1]])\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    Y_pred = pipeline.predict(X_test)\n",
    "    y_prob_output = pipeline.predict_proba(X_test) #this is what we send to kaggle.\n",
    "    result = log_loss(Y_test, y_prob_output)\n",
    "    log_loss_list.append((name, result, pipeline))\n",
    "    print(name)\n",
    "\n",
    "    \n",
    "log_loss_list.sort(key=lambda tup: tup[1], reverse = False)\n",
    "log_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "winner = log_loss_list[0]\n",
    "print('The best combination Number, Processing, Model is ' \n",
    "      + winner[0]\n",
    "     + ' with a log_loss score of '\n",
    "     + str(winner[1]))\n",
    "winner[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelinex = Pipeline(memory=None,\n",
    "     steps=[('S_PCA_892', PCA(copy=True, iterated_power='auto', n_components=892, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('LR', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('selectedModel.txt','w') \n",
    " \n",
    "file.write('The best combination Number, Processing, Model is '\n",
    "           + winner[0]\n",
    "           + ' with a log_loss score of '\n",
    "           + str(winner[1]))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion Matrix</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelinex.fit(X_train, Y_train)\n",
    "Y_pred = pipelinex.predict(X_test)\n",
    "y_prob_output = pipelinex.predict_proba(X_test)\n",
    "final_plot_confusion(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Submission to Kaggle</h2>\n",
    "First, we must adapt the features building functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading Test Set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "test.text= test.text.astype(str)\n",
    "\n",
    "#test = test[0:30] #for coding \n",
    "#construction of the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adapting building functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function works for n-grams of characters, pos tokens or words\n",
    "\n",
    "#first argument is the n in n-gram\n",
    "#analysis type is 'word', 'char', token_pos', ...\n",
    "\n",
    "def counting_a_final(a, analysis):\n",
    "    \n",
    "    df_train = train.copy()\n",
    "    df_test  = test.copy()\n",
    "    \n",
    "    #if we are counting words:\n",
    "    if analysis == \"word\" or analysis == \"char\": \n",
    "        \n",
    "        #check the CountVectorizer doc\n",
    "        #we create a Countvectorizer, called bow_transformer\n",
    "        bow_transformer = CountVectorizer(analyzer = analysis,\n",
    "                                      lowercase = True, #we don't care about place in sentence\n",
    "                                      ngram_range = (a, a),\n",
    "                                      stop_words='english')\n",
    "\n",
    "        #we use bow_transformer to fit and transform our training set\n",
    "        messages_bow = bow_transformer.fit_transform(df_train['text'])\n",
    "        \n",
    "        #we use bow_transformer to transform our test set. \n",
    "        #We do not need to train if first because the fitting would recompute the idf, we don't want that\n",
    "        messages_bow_test = bow_transformer.transform(df_test['text'])\n",
    "    \n",
    "    #if we are counting POS:    \n",
    "    elif analysis == \"token_pos\":\n",
    "        \n",
    "        #this is the punctuation we want to keep\n",
    "        punctuation = r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'|\\.|\\,|\\;|\\:|\\$|\\(|\\)|\\--|\\&|\\``|\\'' + PRP$ + WP$\"\n",
    "        #we create a Countvectorizer, called bow_transformer\n",
    "        bow_transformer = CountVectorizer(analyzer = 'word',\n",
    "                                          lowercase = False, #we DO care about place in sentence\n",
    "                                          ngram_range = (a, a),\n",
    "                                          token_pattern =  punctuation, #we DO care about punctuation\n",
    "                                          stop_words='english')\n",
    "        \n",
    "        #we use the transform_tag function to transform the sentence in a sentence of pos tag        \n",
    "        #we use bow_transformer to fit and transform our training set\n",
    "        messages_bow = bow_transformer.fit_transform(df_train['text'].apply(transform_tag))\n",
    "        #we use bow_transformer to transform our test set\n",
    "        #We do not need to train if first because the fitting would recompute the idf, we don't want that\n",
    "        messages_bow_test = bow_transformer.transform(df_test['text'].apply(transform_tag))\n",
    "\n",
    "        \n",
    "    #this is the DataFrame Concerning the regular counting of words\n",
    "    \n",
    "    ##What does this do?\n",
    "    \"\"\"\n",
    "    messages_tfidf = TfidfTransformer().fit_transform(messages_bow)\n",
    "    messages_tfidf_test = TfidfTransformer().transform(messages_bow_test)\n",
    "    names = bow_transformer.get_feature_names()\n",
    "    \n",
    "    \"\"\"\n",
    "    tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "    messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "    messages_tfidf_test = tfidf_transformer.transform(messages_bow_test)\n",
    "    \n",
    "    names = bow_transformer.get_feature_names()\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    return (messages_tfidf, names, messages_tfidf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_bag_a_final(a,  analysis, top_n = 50):\n",
    "    df_bag = train.copy()\n",
    "    df_bag_test = test.copy()\n",
    "    \n",
    "    build = counting_a_final(a, analysis)\n",
    "    \n",
    "    alpha = top_feats_by_class(build[0], df_bag.author, build[1], top_n = top_n)\n",
    "\n",
    "    a = list(alpha[0].feature.values)\n",
    "    b = list(alpha[1].feature.values)\n",
    "    c = list(alpha[2].feature.values)\n",
    "    bag = set(a + b + c)\n",
    "\n",
    "\n",
    "    for w in bag:\n",
    "        vec = build[0][:, build[1].index(w)].toarray()\n",
    "        df_bag[w] = vec\n",
    "\n",
    "        vec_test = build[2][:, build[1].index(w)].toarray()\n",
    "        df_bag_test[w] = vec_test\n",
    "        \n",
    "    df_bag = df_bag.drop(labels = ['text','author'], axis = 1)\n",
    "    df_bag_test = df_bag_test.drop(labels = ['text'], axis = 1)\n",
    "\n",
    "        \n",
    "    return df_bag, df_bag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we build here the 2 feature datasets (one for train, one for test)\n",
    "#should be adapted when we'll add features\n",
    "\n",
    "def build_bunch_train(dataframe):\n",
    "    list_df_tr = [build_meta2(dataframe),\n",
    "                  build_meta1(dataframe),\n",
    "                  build_sensi(dataframe),\n",
    "                  build_bag_a_final(1,'word')[0],\n",
    "                  build_bag_a_final(2,'word')[0],\n",
    "                  build_bag_a_final(3,'word')[0],\n",
    "                  build_bag_a_final(1,'char')[0],\n",
    "                  build_bag_a_final(2,'char')[0],\n",
    "                  build_bag_a_final(3,'char')[0],\n",
    "                  build_bag_a_final(4,'char')[0],\n",
    "                  build_bag_a_final(5,'char')[0],\n",
    "                  build_bag_a_final(6,'char')[0],\n",
    "                  build_bag_a_final(7,'char')[0],\n",
    "                  build_bag_a_final(1, 'token_pos')[0],\n",
    "                  build_bag_a_final(2, 'token_pos')[0],\n",
    "                  build_bag_a_final(3, 'token_pos')[0],\n",
    "                  build_bag_a_final(4, 'token_pos')[0],\n",
    "                  build_bag_a_final(5, 'token_pos')[0],\n",
    "                  build_bag_a_final(6, 'token_pos')[0],\n",
    "                  first_word(dataframe),\n",
    "#                   twofirst_word(dataframe),\n",
    "                  last_word(dataframe),\n",
    "#                   twolast_word(dataframe),\n",
    "                  language(dataframe),\n",
    "                  emotions(dataframe)\n",
    "              ]\n",
    "    bunch = pd.merge(list_df_tr[0], list_df_tr[1])\n",
    "    \n",
    "    for i in range(2, len(list_df_tr)):\n",
    "        alpha = bunch\n",
    "        bunch = pd.merge(alpha, list_df_tr[i], on = 'id')\n",
    "    \n",
    "    return bunch\n",
    "\n",
    "#build_bunch_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_bunch_test(dataframe):\n",
    "    list_df_ts = [build_meta2(dataframe),\n",
    "                  build_meta1(dataframe),\n",
    "                  build_sensi(dataframe),\n",
    "                  build_bag_a_final(1,'word')[1],\n",
    "                  build_bag_a_final(2,'word')[1],\n",
    "                  build_bag_a_final(3,'word')[1],\n",
    "\n",
    "                  build_bag_a_final(1,'char')[1],\n",
    "                  build_bag_a_final(2,'char')[1],\n",
    "                  build_bag_a_final(3,'char')[1],\n",
    "                  build_bag_a_final(4,'char')[1],\n",
    "                  build_bag_a_final(5,'char')[1],\n",
    "                  build_bag_a_final(6,'char')[1],\n",
    "                  build_bag_a_final(7,'char')[1],\n",
    "               \n",
    "                  build_bag_a_final(1, 'token_pos')[1],\n",
    "                  build_bag_a_final(2, 'token_pos')[1],\n",
    "                  build_bag_a_final(3, 'token_pos')[1],\n",
    "                  build_bag_a_final(4, 'token_pos')[1],\n",
    "                  build_bag_a_final(5, 'token_pos')[1],\n",
    "                  build_bag_a_final(6, 'token_pos')[1],\n",
    "                  first_word(dataframe),\n",
    "                  last_word(dataframe),\n",
    "                  language(dataframe),\n",
    "                  emotions(dataframe)\n",
    "                 ]\n",
    "\n",
    "\n",
    "    bunch = pd.merge(list_df_ts[0], list_df_ts[1])\n",
    "    \n",
    "    for i in range(2, len(list_df_ts)):\n",
    "        bunch = pd.merge(bunch, list_df_ts[i], on = 'id')\n",
    "    \n",
    "    return bunch\n",
    "\n",
    "#build_bunch_test(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training over all the Training Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "\n",
    "names = author_list\n",
    "dataframe_train = build_normalization(build_bunch_train(train))\n",
    "del dataframe_train[\"text\"]\n",
    "dataframe_train.to_csv('final_dataframe_train.csv', index=False)\n",
    "array = dataframe_train.values\n",
    "X_train = array[:,2:]\n",
    "Y_train = array[:,1]\n",
    "\n",
    "names = author_list\n",
    "dataframe_test = build_normalization(build_bunch_test(test))\n",
    "del dataframe_test[\"text\"]\n",
    "dataframe_test.to_csv('final_dataframe_test.csv', index=False)\n",
    "array = dataframe_test.values\n",
    "X_test = array[:,1:]\n",
    "#Y_test = array[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = array[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "pipeline = winner[2]\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating the probabilities</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = pipeline.predict(X_test)\n",
    "y_prob_output = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_prob_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CSV File</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_prob_output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submit = pd.DataFrame(columns = ['id',\"EAP\",\"HPL\",\"MWS\"])\n",
    "df_submit['id'] = test['id']\n",
    "df_submit['EAP'] = y_prob_output[:,0]\n",
    "df_submit['HPL'] = y_prob_output[:,1]\n",
    "df_submit['MWS'] = y_prob_output[:,2]\n",
    "df_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "file = open('time.txt','w') \n",
    "file.write('Congratulations, the project is done! It took: ' + str(end_time - start_time))\n",
    " \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conclusion\n",
    "[We need to add a conclusion]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
